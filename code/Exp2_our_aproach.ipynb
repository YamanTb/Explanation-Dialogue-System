{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c508efad-1eb4-4796-84eb-3dc737a19e93",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "In this notebook, we provide our approach for our experiment, which involves explaining machine through planning and realizing explanation moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa227bf4-e62d-407a-a7a8-89c8dc92eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required packages\n",
    "!pip install bitsandbytes\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
    "!pip uninstall peft -y\n",
    "!pip install -q git+https://github.com/huggingface/peft.git@0769587a3cd80ad2ae508cc06efbf54ddca821b3\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bafbce-dd3f-4af7-8379-4aa51fb46f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a09c00-2e14-43a1-860f-2c128436fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the CRF model\n",
    "#Model is based on the work of Wchasmuth and Alshomary (A Dialogue Corpus for Learning to Construct Explanations)\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, './lib/src')\n",
    "\n",
    "import torch\n",
    "from multi_turn_bert import MultiTurnBert\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = argparse.Namespace(turn_type='multi', pooling='cls', sp1_token='[EXPLAINER]', sp2_token='[EXPLAINEE]', bert_type='bert-base-uncased',\n",
    "                          max_len=256, max_turns=56, dropout=0.1, device='cuda', learning_rate=2e-5, warmup_ratio=0.01,\n",
    "                          batch_size=1, num_workers=2, num_epochs=5, num_classes=-1, ckpt_dir='./cross-val-models', planning=False, start_token='[START]')\n",
    "\n",
    "# Device setting\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA is unavailable. Starting with CPU.\")\n",
    "    args.device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# Adding arguments\n",
    "bert_config = BertConfig().from_pretrained(args.bert_type)\n",
    "args.hidden_size = bert_config.hidden_size\n",
    "args.p_dim = args.hidden_size\n",
    "args.max_len = min(args.max_len, bert_config.max_position_embeddings)\n",
    "\n",
    "\n",
    "tokenizer_crf = BertTokenizer.from_pretrained(args.bert_type)\n",
    "\n",
    "num_new_tokens = tokenizer_crf.add_special_tokens(\n",
    "    {\n",
    "        'additional_special_tokens': [args.sp1_token, args.sp2_token, args.start_token]\n",
    "    }\n",
    ")\n",
    "\n",
    "vocab = tokenizer_crf.get_vocab()\n",
    "args.vocab_size = len(vocab)\n",
    "\n",
    "args.cls_token = tokenizer_crf.cls_token\n",
    "args.sep_token = tokenizer_crf.sep_token\n",
    "args.pad_token = tokenizer_crf.pad_token\n",
    "\n",
    "args.cls_id = vocab[args.cls_token]\n",
    "args.sep_id = vocab[args.sep_token]\n",
    "args.pad_id = vocab[args.pad_token]\n",
    "args.sp1_id = vocab[args.sp1_token]\n",
    "args.sp2_id = vocab[args.sp2_token]\n",
    "args.start_id = vocab[args.start_token]\n",
    "args.o_id   = -1\n",
    "\n",
    "bert_config = BertConfig().from_pretrained(args.bert_type)\n",
    "args.hidden_size = bert_config.hidden_size\n",
    "args.p_dim = args.hidden_size\n",
    "args.max_len = min(args.max_len, bert_config.max_position_embeddings)\n",
    "\n",
    "#planning = false -> with last text\n",
    "#plannung = true -> without last text\n",
    "args.num_classes=10\n",
    "model_exp_with_last_text = MultiTurnBert(args).to(args.device)\n",
    "model_exp_with_last_text.load_state_dict(torch.load('./exp_act_models/ckpt_epoch=1_train_f1=0.8253_valid_f1=0.414_False_with_text'))\n",
    "model_exp_with_last_text.eval()\n",
    "\n",
    "args.planning = True\n",
    "model_exp_without_last_text = MultiTurnBert(args).to(args.device)\n",
    "model_exp_without_last_text.load_state_dict(torch.load('./exp_act_models/ckpt_epoch=3_train_f1=0.9256_valid_f1=0.424_True_without_text'))\n",
    "model_exp_without_last_text.eval()\n",
    "\n",
    "\n",
    "args.planning = False\n",
    "args.num_classes=4\n",
    "model_topic_with_last_text = MultiTurnBert(args).to(args.device)\n",
    "model_topic_with_last_text.load_state_dict(torch.load('./topic_rel_models/ckpt_epoch=1_train_f1=0.8155_valid_f1=0.5814_False_with_text'))\n",
    "model_topic_with_last_text.eval()\n",
    "\n",
    "args.planning = True\n",
    "model_topic_without_last_text = MultiTurnBert(args).to(args.device)\n",
    "model_topic_without_last_text.load_state_dict(torch.load('./topic_rel_models/ckpt_epoch=4_train_f1=0.9125_valid_f1=0.5385_True_without_text'))\n",
    "model_topic_without_last_text.eval()\n",
    "\n",
    "#planning = false -> with last text\n",
    "#plannung = true -> without last text\n",
    "def predict_label(dig, args, is_exp, planning):\n",
    "    args.planning = planning\n",
    "    args.num_classes = 10 if is_exp else 4\n",
    "    label_clm = 'exp_act_label' if is_exp else 'topic_func_label'\n",
    "    dig['label'] = dig[label_clm]#.apply(lambda labels: [int(x[2:4])-1 for x in labels])\n",
    "    model = model_exp_with_last_text if is_exp and not planning else\\\n",
    "        \tmodel_exp_without_last_text if is_exp and planning else\\\n",
    "            model_topic_with_last_text if not is_exp and not planning else\\\n",
    "            model_topic_without_last_text\n",
    "    ds = CustomDataset(dig, tokenizer_crf, args, vocab)\n",
    "    ds_loader = DataLoader(ds, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(ds_loader):\n",
    "            batch_x, batch_turns, batch_labels, true_turn_lens = batch\n",
    "            batch_x, batch_turns, batch_labels = \\\n",
    "                batch_x.to(args.device), batch_turns.to(args.device), batch_labels.to(args.device)\n",
    "            _, outputs = model(batch_x, args.pad_id, batch_turns, turns_labels=batch_labels) \n",
    "    return outputs[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af697c2-2cb4-4e45-8035-1dd8d7e8d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code loads the Alpaca LoRA pretrained model.\n",
    "from peft import PeftModel\n",
    "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(\"linhvu/decapoda-research-llama-7b-hf\")\n",
    "model = LLaMAForCausalLM.from_pretrained(\"linhvu/decapoda-research-llama-7b-hf\", load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85770fd2-ed05-4962-851f-cc37d5f397c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup of Alpaca\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca392b-38fe-4d31-b2ab-cbd62bd02d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate function that we will use to call Alpaca LoRA.\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    "    repetition_penalty=1.03\n",
    ")\n",
    "\n",
    "def evaluate(instruction, input=None):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    response = []\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        response.append(output.split(\"### Response:\")[1].strip())\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e1cc9-a92c-44ca-a061-64322a03de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and prepare the data for the CRF model\n",
    "df = pd.read_pickle('./data_turns.pkl')\n",
    "df['turn_len'] = df.turn_text.apply(lambda x: len(x['text'].split()))\n",
    "df['turn_text'] = df.apply(lambda row: {\n",
    "                                            'author': row['turn_text']['author'], \n",
    "                                            'text' : row['topic'].replace('_', ' ') + ' [SEP] ' +  row['turn_text']['text']\n",
    "                                           } ,axis=1)\n",
    "df = df.groupby('task_id').agg({'turn_text': lambda rows: list(rows),\n",
    "                                'topic': lambda rows: list(rows)[0],\n",
    "                                'topic_func_label': lambda rows: list(rows),\n",
    "                                'dlg_act_label': lambda rows: list(rows),\n",
    "                                'exp_act_label': lambda rows: list(rows)}).reset_index()\n",
    "df['num_turns_per_topic'] = df.topic_func_label.apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5b89c-2337-4067-bf71-28d5e59e348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data in training set 60%, validation set 20%, test set 20%\n",
    "_, valid, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "valid_df = valid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7c4f0-6bb5-4599-8c9e-ad11aa630d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamplate of questions for the label e1 Testunderstanding of explanation move dimention \n",
    "test_understanding_questions = [\n",
    "    \"Did that explanation make sense to you?\",\n",
    "    \"Was my last point clear enough?\",\n",
    "    \"Do you feel confident about what we just discussed?\",\n",
    "    \"Do you think you can summarize the main idea?\",\n",
    "    \"Can you recall the key points we covered?\",\n",
    "    \"Was the information I provided relevant to your interests?\",\n",
    "    \"Do you agree with the information presented?\",\n",
    "    \"Do you have any different views on this topic?\",\n",
    "    \"Is there anything you would challenge or question about what we discussed?\",\n",
    "    \"Are you finding this topic interesting?\",\n",
    "    \"Do you want to learn more about this subject?\",\n",
    "    \"Has this discussion sparked any new ideas or thoughts for you?\",\n",
    "    \"Is there anything you'd like me to explain differently?\",\n",
    "    \"Do you think the examples I used were helpful?\",\n",
    "    \"Would you like to go over any part of the topic again?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e88d7d-854f-4ff9-8d4f-df5506935bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random question to the label e1 Testunderstanding\n",
    "import random\n",
    "\n",
    "def get_random_question(question_list):\n",
    "    return random.choice(question_list)\n",
    "\n",
    "# Example usage\n",
    "random_test_understanding = get_random_question(test_understanding_questions)\n",
    "print(random_test_understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acd978-8f08-4bf9-ba29-2a273ed3a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamplate of questions for e2 Test Prior Knowledge  of explanation move dimention  regarding Topic dimention\n",
    "\n",
    "test_prior_knowledge_questions = {\n",
    "\n",
    "    1 : [ # Main_topic:\n",
    "      \"So do you know what [] is?\",\n",
    "      \"Do you know what [] is?\",\n",
    "      \"Have you ever heard of [] before?\",\n",
    "      \"Do you know what [] means? Have you heard that before?\",\n",
    "      \"So have you ever heard of something called []?\",\n",
    "      \"Have you heard about [] before?\",\n",
    "      \"Are you familiar with the basics of []?\",\n",
    "      \"Have you encountered [] before?\",\n",
    "    ],\n",
    "    2 : [ # related_topic:\n",
    "      \"So do you have a basic sense of what [] is?\",\n",
    "      \"Have you read or learned anything specific about []?\",\n",
    "      \"How does [] relate to what you already know?\",\n",
    "      \"Do you know any applications of [] in real-world scenarios?\",\n",
    "      \"What do you already know about []?\",\n",
    "      \"In what context have you encountered [] previously?\",\n",
    "      \"What are your thoughts or opinions about []?\",\n",
    "      \"Does [] interest you in any particular way?\"\n",
    "    ],\n",
    "\n",
    "    3 : [ # sub_topic:\n",
    "      \"So do you have a basic sense of what [] is?\",\n",
    "      \"Have you read or learned anything specific about []?\",\n",
    "      \"How does [] relate to what you already know?\",\n",
    "      \"Do you know any applications of [] in real-world scenarios?\",\n",
    "      \"What do you already know about []?\",\n",
    "      \"In what context have you encountered [] previously?\",\n",
    "      \"What are your thoughts or opinions about []?\",\n",
    "      \"Does [] interest you in any particular way?\"\n",
    "    ],\n",
    "\n",
    "    4 : [ # other_topic:\n",
    "      \"And you know what we're going to be talking about?\",\n",
    "      \"So what's your major?\",\n",
    "      \"Do you think of yourself as curious about science?\",\n",
    "      \"Are you interested in science?\",\n",
    "      \"Do you like science? Is that something you think about?\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851e06f-cbf0-449c-be42-7049e913bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the needed Prompts based on the main topic for realizing explanation moves as responses from Alpaca\n",
    "def generate_prompts(main_topic):\n",
    "    \n",
    "    E2_RELATED_TOPIC_PROMPT = (\n",
    "        f'''\n",
    "        Identify three basic, foundational concepts related to \"{main_topic}\" suitable for placeholders in questions.\n",
    "        Present them in an enumerated format.\n",
    "        Each concept should be expressed as a single word or a very brief phrase, ensuring simplicity and direct relevance to \"{main_topic}\".\n",
    "        These concepts should be basic enough to be understood by beginners in the subject.\n",
    "        '''\n",
    "    )\n",
    "    E3_EXPLANATION_PROMPT = (\n",
    "        f'''\n",
    "        Based on the following dialogue history between an explainer an explainee, play the role of the explainer,\n",
    "        generate a response that provides a clear, concise, and informative explanation about the main topic \"{main_topic}\" if needed or the topics discussed in the previous turns.\n",
    "        The response should logically continue the conversation and make sense in the context of the dialogue.\n",
    "        Focus on delivering an explanation that enhances understanding of the main topic or related concepts.\n",
    "        The response should be in plain text format, without any dialogue format or author labels.\n",
    "        '''\n",
    "    )\n",
    "    \n",
    "    E4_FEEDBACK_PROMPT = (\n",
    "        f'''\n",
    "        After analyzing the entire dialogue about the main topic \"{main_topic}\",\n",
    "        play the role of the explainer in the next turn and provide a the next turn as a feedback response that addresses the explainee’s current understanding.\n",
    "        The response should reflect on the progression of the explainee’s comprehension throughout the dialogue and focus on their most recent response.\n",
    "        The output must be a standalone response, only the turn text content of the explainer, without repeating previous dialogue or turns from it.\n",
    "        '''\n",
    "    )\n",
    "\n",
    "    E5_OTHER_MOVE_PROMPT = (\n",
    "        f'''\n",
    "        Based on the full dialogue history provided below about the main topic \"{main_topic}\",\n",
    "        generate a relevant and coherent response as the explainer.\n",
    "        This response should appropriately continue the conversation,\n",
    "        addressing any new points or questions raised in the entire dialogue,\n",
    "        and adding value to the discussion in a way that is not strictly explanatory or feedback-oriented.\n",
    "        Ensure the response is engaging, contextually aware, and maintains the flow of the dialogue.\n",
    "        '''\n",
    "    )\n",
    "    BASELINE_PRPOMT = (\n",
    "        f'''\n",
    "        Given the dialogue provided between an explainer and an explainee, generate the next turn for the explainer.\n",
    "        The response should be contextually aware, directly addressing the previous turn by the explainee.\n",
    "        It should continue the theme of the conversation in an engaging and informative manner, ensuring that the dialogue flows naturally.\n",
    "        The explainer's response must be relevant to the topic discussed and should aim to further the conversation, providing insights, clarifications, or thoughtful questions as appropriate.\n",
    "        Ensure the tone is friendly and conducive to a positive and constructive exchange of ideas.\n",
    "        '''\n",
    "    )\n",
    "    return E2_RELATED_TOPIC_PROMPT, E3_EXPLANATION_PROMPT, E4_FEEDBACK_PROMPT, E5_OTHER_MOVE_PROMPT, BASELINE_PRPOMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107a0e4-ad1d-49dd-8c37-764a5b05acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract relevant text from a response string, clearing away unnecessary content.\n",
    "def extract_turn_text(response):\n",
    "    start_index = response.find(\"turn text:\")\n",
    "    if start_index != -1:\n",
    "        text_after_start = response[start_index + 10:]\n",
    "        return text_after_start.split('\\n', 1)[0].strip()\n",
    "    \n",
    "    start_index1 = response.find(\"Explainer:\")\n",
    "    if start_index1 != -1:\n",
    "        text_after_start1 = response[start_index1 + 10:]\n",
    "        return text_after_start1.split('\\n', 1)[0].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be17aeb-1f49-417e-8d98-0e480a32c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a response using Alpaca's prompts\n",
    "def generate_response_e3_e4_e5(PROMPT,dig,i):\n",
    "    To_print = ''\n",
    "    INPUT = ''\n",
    "    for j in range(i):\n",
    "        # Find the position of '[SEP]' in the text\n",
    "        sep_position = dig.turn_text.iloc[0][j]['text'].find('[SEP]')\n",
    "        if sep_position != -1:\n",
    "            # Extract text after '[SEP]'\n",
    "            text_after_sep = dig.turn_text.iloc[0][j]['text'][sep_position + len('[SEP]'):].strip()\n",
    "        else:\n",
    "            # If '[SEP]' is not found, use the original text\n",
    "            text_after_sep = dig.turn_text.iloc[0][j]['text']\n",
    "        \n",
    "        INPUT += f\"turn {j+1}, author: {dig.turn_text.iloc[0][j]['author']}, turn text: {text_after_sep}.\\n\" # turn 1, author: Explainee, turn text: sleep_scientist.\n",
    "        To_print += f\"{j+1}, {dig.turn_text.iloc[0][j]['author']}:, {text_after_sep}.\\n\" # 1, Explainee:, sleep_scientist.\n",
    "        # To_print += '- ' + text_after_sep + '\\n' # - sleep_scientist\n",
    "        \n",
    "    print(INPUT)    \n",
    "    response = evaluate(PROMPT,INPUT)\n",
    "    return extract_turn_text(response[0]) if response else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15481325-3d77-4241-86c9-6c675c457000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for and realize the explanation move for one turn in a dialogue \n",
    "def realization_response(dig, main_topic, E2_RELATED_TOPIC_PROMPT, E3_EXPLANATION_PROMPT, E4_FEEDBACK_PROMPT, E5_OTHER_MOVE_PROMPT):   \n",
    "    \n",
    "    curr_turn = len(dig.turn_text[0])\n",
    "    # first turn from explainer in 5 levels:\n",
    "    if dig.task_id.iloc[0] < 207 and curr_turn < 3:\n",
    "        response = get_random_question(test_prior_knowledge_questions[1]).replace(\"[]\", main_topic)\n",
    "        print(response)\n",
    "        return response\n",
    "    \n",
    "    # first turn from explainer in reddit:\n",
    "    elif dig.task_id.iloc[0] > 207 and curr_turn < 3:\n",
    "        response = generate_response_e3_e4_e5(E3_EXPLANATION_PROMPT, dig, curr_turn) \n",
    "        print(response)\n",
    "        return response\n",
    "    \n",
    "    # Ab Turn 4:\n",
    "    #connect with CRF\n",
    "    #def predict_label(dig, is_exp(false = topic), planning)\n",
    "    #planning = false -> with last text\n",
    "    #plannung = true -> without last text\n",
    "    \n",
    "    crf_exp_act_result = predict_label(dig, args, True, True)\n",
    "    crf_topic_result = predict_label(dig, args, False, True)\n",
    "    exp_act_label = CRF_output_exp_act_label_Dic[crf_exp_act_result]\n",
    "    topic_label = CRF_output_topic_func_label_Dic[crf_topic_result]\n",
    "    print ('topic label:', topic_label, 'exp_label: ', exp_act_label)\n",
    "\n",
    "    if exp_act_label == 1: # Test understanding\n",
    "         response = get_random_question(test_understanding_questions)\n",
    "    elif exp_act_label == 2: # Test prior knowledge\n",
    "        if topic_label == 1:\n",
    "            response = get_random_question(test_prior_knowledge_questions[topic_label]).replace(\"[]\", main_topic)\n",
    "        else:  \n",
    "            related_topic = generate_related_topic(E2_RELATED_TOPIC_PROMPT)\n",
    "            response = get_random_question(test_prior_knowledge_questions[topic_label]).replace(\"[]\", related_topic)\n",
    "    elif exp_act_label == 3: # Provide Explanation \n",
    "        response = generate_response_e3_e4_e5(E3_EXPLANATION_PROMPT, dig, curr_turn)\n",
    "    elif exp_act_label == 4: # Provide Feedback\n",
    "        response = generate_response_e3_e4_e5(E4_FEEDBACK_PROMPT, dig, curr_turn)\n",
    "    else: # e5 Other\n",
    "        response = generate_response_e3_e4_e5(E5_OTHER_MOVE_PROMPT, dig, curr_turn)\n",
    "   \n",
    "    response = extract_turn_text(response)\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5547d-4d1f-4547-8e76-099f47e77539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment on one dialogue and save the results\n",
    "def realization_dialog(original_dig):\n",
    "    results = []\n",
    "    main_topic = original_dig.topic\n",
    "    E2_RELATED_TOPIC_PROMPT, E3_EXPLANATION_PROMPT, E4_FEEDBACK_PROMPT, E5_OTHER_MOVE_PROMPT, BASELINE_PRPOMT = generate_prompts(main_topic)\n",
    "    cos_list_approach = []\n",
    "    cos_list_basline = []\n",
    "\n",
    "    \n",
    "    dig = pd.DataFrame([{\n",
    "            'task_id': original_dig.task_id, \n",
    "            'turn_text': original_dig.turn_text[0],\n",
    "            'topic': main_topic,\n",
    "            'topic_func_label': int(original_dig.topic_func_label[0][2:4]) - 1,\n",
    "            'exp_act_label': int(original_dig.exp_act_label[0][2:4]) - 1,\n",
    "        }])\n",
    "    dig = dig.groupby('task_id').agg({'turn_text': lambda rows: list(rows),\n",
    "                                    'topic': lambda rows: list(rows)[0],\n",
    "                                    'topic_func_label': lambda rows: list(rows),\n",
    "                                    'exp_act_label': lambda rows: list(rows)}).reset_index()\n",
    "    \n",
    "    for i in range(1, original_dig.num_turns_per_topic):\n",
    "        if original_dig.turn_text[i]['author'] == 'Explainer':\n",
    "            response = realization_response(dig, main_topic, E2_RELATED_TOPIC_PROMPT, E3_EXPLANATION_PROMPT, E4_FEEDBACK_PROMPT, E5_OTHER_MOVE_PROMPT)\n",
    "            gt = original_dig.turn_text[i]['text'].split(' [SEP] ')[-1]\n",
    "            response_basline = generate_response_e3_e4_e5(BASELINE_PRPOMT, dig, i)\n",
    "            response_basline_encoded = model_similarity.encode(response_basline)\n",
    "            response_encoded = model_similarity.encode(response)\n",
    "            original_encoded = model_similarity.encode(gt)\n",
    "            \n",
    "            # Results aren't great with the all-distilroberta-v1 model\n",
    "            cos_sim_approach = util.cos_sim(response_encoded, original_encoded)\n",
    "            cos_sim_baseline = util.cos_sim(response_basline_encoded, original_encoded)\n",
    "            cos_list_approach.append(cos_sim_approach)\n",
    "            cos_list_basline.append(cos_sim_baseline)\n",
    "            print('#' * 20)\n",
    "            print('Reponse:', response)\n",
    "            print('Response Basline:', response_basline)\n",
    "            print('Ground truth:', gt)\n",
    "            print(\"Similarity approach:\", cos_sim_approach)\n",
    "            print(\"Similarity baseline:\", cos_sim_baseline)\n",
    "            print('#' * 20)\n",
    "\n",
    "            results.append([i, gt, response, response_basline, cos_sim_approach, cos_sim_baseline])\n",
    "            \n",
    "        dig.turn_text.iloc[0].append(original_dig.turn_text[i])\n",
    "        dig.topic_func_label.iloc[0].append(int(original_dig.topic_func_label[i][2:4]) - 1)\n",
    "        dig.exp_act_label.iloc[0].append(int(original_dig.exp_act_label[i][2:4]) - 1)\n",
    "    return original_dig.task_id, sum(cos_list_basline) / len(cos_list_basline),  sum(cos_list_approach) / len(cos_list_approach), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93366756-b5ad-44c5-adae-3d496813d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment on the validation set and save the results in a data frame\n",
    "valid_results = []\n",
    "for i in range(86, 93):\n",
    "    print('*' * 20, i, '*' * 20)\n",
    "    valid_results.append(realization_dialog(valid.iloc[i]))\n",
    "df_valid_results = pd.DataFrame(valid_results, columns=['task_id', 'avg_cos_sim_baseline', 'avg_cos_sim_approach', 'results'])\n",
    "df_valid_results.to_pickle('./valid_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fc8ef-3165-4af9-a938-96cb382d41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment on the test set and save the results in a data frame\n",
    "test_results = [] \n",
    "for i in range(93):\n",
    "    print('*' * 20, i, '*' * 20)\n",
    "    test_results.append(realization_dialog(test.iloc[i]))\n",
    "df_test_results = pd.DataFrame(test_results, columns=['task_id', 'avg_cos_sim_baseline', 'avg_cos_sim_approach', 'results'])\n",
    "df_test_results.to_pickle('./test_results.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
